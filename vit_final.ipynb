{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syavari/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Client is not authorized to connect to Server"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification, ViTConfig, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "         Image Index                            FilePath  Effusion  \\\n",
      "2   00000001_002.png  images_001/images/00000001_002.png         1   \n",
      "7   00000003_003.png  images_001/images/00000003_003.png         0   \n",
      "19  00000005_006.png  images_001/images/00000005_006.png         0   \n",
      "25  00000008_002.png  images_001/images/00000008_002.png         0   \n",
      "27  00000010_000.png  images_001/images/00000010_000.png         0   \n",
      "\n",
      "    Infiltration  Mass  Nodule  Atelectasis  Pneumothorax  Total Diseases  \n",
      "2              0     0       0            0             0               1  \n",
      "7              1     0       0            0             0               1  \n",
      "19             1     0       0            0             0               1  \n",
      "25             0     0       1            0             0               1  \n",
      "27             1     0       0            0             0               1  \n",
      "Total images in dataset: 31085\n",
      "Dataset saved to ./output_directory/train_df_main_full.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syavari/.local/lib/python3.8/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7780' max='7780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7780/7780 4:41:36, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.400400</td>\n",
       "      <td>0.410731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.407900</td>\n",
       "      <td>0.407516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.420700</td>\n",
       "      <td>0.409602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.404200</td>\n",
       "      <td>0.409030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.407800</td>\n",
       "      <td>0.405485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.400800</td>\n",
       "      <td>0.401431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.385400</td>\n",
       "      <td>0.403665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.394600</td>\n",
       "      <td>0.398229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.392000</td>\n",
       "      <td>0.397405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.402467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.394600</td>\n",
       "      <td>0.396975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.376100</td>\n",
       "      <td>0.396626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.375400</td>\n",
       "      <td>0.403389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.365200</td>\n",
       "      <td>0.408620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.360300</td>\n",
       "      <td>0.418617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.337900</td>\n",
       "      <td>0.439594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.319900</td>\n",
       "      <td>0.459445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.286600</td>\n",
       "      <td>0.499617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.528112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.229500</td>\n",
       "      <td>0.565458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # \"cuda:0\" will refer to GPU 3\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "file_path = 'train_df.csv'\n",
    "train_df_main = pd.read_csv(file_path)\n",
    "\n",
    "# Map image paths to the DataFrame\n",
    "all_image_paths = {os.path.basename(x): x for x in glob(os.path.join('images*', '*', '*.png'))}\n",
    "train_df_main[\"FilePath\"] = train_df_main[\"Image Index\"].map(all_image_paths)\n",
    "\n",
    "# Drop 'No Finding' column\n",
    "train_df_main.drop(['No Finding'], axis=1, inplace=True)\n",
    "\n",
    "# Selected labels\n",
    "selected_labels = ['Effusion', 'Infiltration', 'Mass', 'Nodule', 'Atelectasis', 'Pneumothorax']\n",
    "\n",
    "# Filter for selected labels\n",
    "train_df_main = train_df_main[['Image Index', 'FilePath'] + selected_labels]\n",
    "\n",
    "# Calculate the sum of diseases per patient\n",
    "train_df_main['Total Diseases'] = train_df_main[selected_labels].sum(axis=1)\n",
    "\n",
    "# Filter patients with exactly one disease\n",
    "one_disease_df = train_df_main[train_df_main['Total Diseases'] == 1]\n",
    "\n",
    "# Calculate the frequency of each disease\n",
    "disease_counts = one_disease_df[selected_labels].sum()\n",
    "\n",
    "# Filter diseases with at least 2000 samples\n",
    "frequent_diseases = disease_counts[disease_counts >= 2000].index.tolist()\n",
    "\n",
    "# Filter the DataFrame for these frequent diseases\n",
    "train_df_main = one_disease_df[one_disease_df[frequent_diseases].eq(1).any(axis=1)]\n",
    "\n",
    "# Check if the subset was created correctly\n",
    "print(train_df_main.head())\n",
    "print(f\"Total images in dataset: {len(train_df_main)}\")\n",
    "\n",
    "# Specify the directory where you want to save the file\n",
    "output_directory = './output_directory'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = os.path.join(output_directory, 'train_df_main_full.csv')\n",
    "\n",
    "# Save to a new CSV (optional)\n",
    "try:\n",
    "    train_df_main.to_csv(output_file_path, index=False)\n",
    "    print(f\"Dataset saved to {output_file_path}\")\n",
    "except PermissionError as e:\n",
    "    print(f\"PermissionError: {e}. Could not save the file at {output_file_path}\")\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = os.path.join(output_directory, 'subset_df_new.csv')\n",
    "\n",
    "# Define custom dataset for ViT\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, labels, transform=None):\n",
    "        self.df = df\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.image_paths = df['FilePath'].values\n",
    "        self.label_values = df[labels].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.label_values[idx], dtype=torch.float32)\n",
    "        return {\"pixel_values\": image, \"labels\": label}\n",
    "\n",
    "# Custom transform function\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create train-test split\n",
    "train_df, test_df = train_test_split(train_df_main, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomImageDataset(train_df, frequent_diseases, transform=transform)\n",
    "test_dataset = CustomImageDataset(test_df, frequent_diseases, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # Increased batch size\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model from scratch\n",
    "config = ViTConfig(\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    num_labels=len(frequent_diseases)\n",
    ")\n",
    "model = ViTForImageClassification(config).to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,  \n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(outputs.logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def compute_metrics(self, p):\n",
    "        preds = torch.sigmoid(p.predictions).cpu().numpy()\n",
    "        labels = p.label_ids\n",
    "        preds = (preds > 0.5).astype(int)\n",
    "        accuracy = (preds == labels).mean()\n",
    "        return {\"accuracy\": accuracy}\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./trained_model')\n",
    "\n",
    "# # Evaluation on test set\n",
    "# def evaluate_model(trainer, dataset):\n",
    "#     trainer.model.eval()  # Set the model to evaluation mode\n",
    "#     predictions, labels = [], []\n",
    "#     for batch in DataLoader(dataset, batch_size=32):\n",
    "#         inputs = {\"pixel_values\": batch[\"pixel_values\"].to(device)}\n",
    "#         with torch.no_grad():\n",
    "#             outputs = trainer.model(**inputs)\n",
    "#         logits = outputs.logits.detach().cpu().numpy()\n",
    "#         predictions.append(logits)\n",
    "#         labels.append(batch[\"labels\"].numpy())\n",
    "#     predictions = np.concatenate(predictions, axis=0)\n",
    "#     labels = np.concatenate(labels, axis=0)\n",
    "#     predictions = (predictions > 0.5).astype(int)\n",
    "#     acc = accuracy_score(labels, predictions)\n",
    "#     f1 = f1_score(labels, predictions, average='macro')\n",
    "#     return predictions, labels, acc, f1\n",
    "\n",
    "# # Evaluate the model\n",
    "# predictions, labels, accuracy, f1 = evaluate_model(trainer, test_dataset)\n",
    "# print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "# # Classification report\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(labels, predictions, target_names=frequent_diseases))\n",
    "\n",
    "# # Plot all ROC curves in one graph\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# for i, label in enumerate(frequent_diseases):\n",
    "#     fpr, tpr, _ = roc_curve(labels[:, i], predictions[:, i])\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "#     plt.plot(fpr, tpr, lw=2, label=f'ROC curve of {label} (area = {roc_auc:.2f})')\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic for all labels')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developed Model with pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and then trained again on out data set to achieve better metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "         Image Index                            FilePath  Effusion  \\\n",
      "2   00000001_002.png  images_001/images/00000001_002.png         1   \n",
      "7   00000003_003.png  images_001/images/00000003_003.png         0   \n",
      "19  00000005_006.png  images_001/images/00000005_006.png         0   \n",
      "25  00000008_002.png  images_001/images/00000008_002.png         0   \n",
      "27  00000010_000.png  images_001/images/00000010_000.png         0   \n",
      "\n",
      "    Infiltration  Mass  Nodule  Atelectasis  Pneumothorax  Total Diseases  \n",
      "2              0     0       0            0             0               1  \n",
      "7              1     0       0            0             0               1  \n",
      "19             1     0       0            0             0               1  \n",
      "25             0     0       1            0             0               1  \n",
      "27             1     0       0            0             0               1  \n",
      "Total images in dataset: 31085\n",
      "Dataset saved to ./output_directory/train_df_main_full.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/syavari/.local/lib/python3.8/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1471' max='7780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1471/7780 45:58 < 3:17:27, 0.53 it/s, Epoch 3.78/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.424400</td>\n",
       "      <td>0.429270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.393400</td>\n",
       "      <td>0.394242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.367892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 136\u001b[0m\n\u001b[1;32m    128\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m    129\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    130\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m    131\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m    132\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m    133\u001b[0m )\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m    139\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./trained_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:2178\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2175\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2177\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2179\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/data_loader.py:464\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[0;32m--> 464\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 68\u001b[0m, in \u001b[0;36mCustomImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     67\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx]\n\u001b[0;32m---> 68\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     70\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/PIL/Image.py:873\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m    832\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n\u001b[1;32m    877\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpalette:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/PIL/ImageFile.py:251\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(b)\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    250\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 251\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # \"cuda:0\" will refer to GPU 3\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "file_path = 'train_df.csv'\n",
    "train_df_main = pd.read_csv(file_path)\n",
    "\n",
    "# Map image paths to the DataFrame\n",
    "all_image_paths = {os.path.basename(x): x for x in glob(os.path.join('images*', '*', '*.png'))}\n",
    "train_df_main[\"FilePath\"] = train_df_main[\"Image Index\"].map(all_image_paths)\n",
    "\n",
    "# Drop 'No Finding' column\n",
    "train_df_main.drop(['No Finding'], axis=1, inplace=True)\n",
    "\n",
    "# Selected labels\n",
    "selected_labels = ['Effusion', 'Infiltration', 'Mass', 'Nodule', 'Atelectasis', 'Pneumothorax']\n",
    "\n",
    "# Filter for selected labels\n",
    "train_df_main = train_df_main[['Image Index', 'FilePath'] + selected_labels]\n",
    "\n",
    "# Calculate the sum of diseases per patient\n",
    "train_df_main['Total Diseases'] = train_df_main[selected_labels].sum(axis=1)\n",
    "\n",
    "# Filter patients with exactly one disease\n",
    "one_disease_df = train_df_main[train_df_main['Total Diseases'] == 1]\n",
    "\n",
    "# Calculate the frequency of each disease\n",
    "disease_counts = one_disease_df[selected_labels].sum()\n",
    "\n",
    "# Filter diseases with at least 2000 samples\n",
    "frequent_diseases = disease_counts[disease_counts >= 2000].index.tolist()\n",
    "\n",
    "# Filter the DataFrame for these frequent diseases\n",
    "train_df_main = one_disease_df[one_disease_df[frequent_diseases].eq(1).any(axis=1)]\n",
    "\n",
    "print(train_df_main.head())\n",
    "print(f\"Total images in dataset: {len(train_df_main)}\")\n",
    "\n",
    "output_directory = './output_directory'\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "output_file_path = os.path.join(output_directory, 'train_df_main_full.csv')\n",
    "\n",
    "try:\n",
    "    train_df_main.to_csv(output_file_path, index=False)\n",
    "    print(f\"Dataset saved to {output_file_path}\")\n",
    "except PermissionError as e:\n",
    "    print(f\"PermissionError: {e}. Could not save the file at {output_file_path}\")\n",
    "\n",
    "# Define custom dataset for ViT\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, labels, transform=None):\n",
    "        self.df = df\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.image_paths = df['FilePath'].values\n",
    "        self.label_values = df[labels].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.label_values[idx], dtype=torch.float32)\n",
    "        return {\"pixel_values\": image, \"labels\": label}\n",
    "\n",
    "# Custom transform function with data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create train-test split\n",
    "train_df, test_df = train_test_split(train_df_main, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomImageDataset(train_df, frequent_diseases, transform=transform)\n",
    "test_dataset = CustomImageDataset(test_df, frequent_diseases, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  \n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model with pre-trained weights\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=len(frequent_diseases)).to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=20,  \n",
    "    per_device_train_batch_size=64,\n",
    "    learning_rate=2e-5,  \n",
    "    warmup_steps=1000,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_steps=10,\n",
    "    load_best_model_at_end=True, )\n",
    "\n",
    "# Define Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(outputs.logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def compute_metrics(self, p):\n",
    "        preds = torch.sigmoid(p.predictions).cpu().numpy()\n",
    "        labels = p.label_ids\n",
    "        preds = (preds > 0.5).astype(int)\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        f1 = f1_score(labels, preds, average='macro')\n",
    "        return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./trained_model')\n",
    "\n",
    "# # Evaluation on test set\n",
    "# def evaluate_model(trainer, dataset):\n",
    "#     trainer.model.eval()  \n",
    "#     predictions, labels = [], []\n",
    "#     for batch in DataLoader(dataset, batch_size=64):  # Match batch size with training\n",
    "#         inputs = {\"pixel_values\": batch[\"pixel_values\"].to(device)}\n",
    "#         with torch.no_grad():\n",
    "#             outputs = trainer.model(**inputs)\n",
    "#         logits = outputs.logits.detach().cpu().numpy()\n",
    "#         predictions.append(logits)\n",
    "#         labels.append(batch[\"labels\"].numpy())\n",
    "#     predictions = np.concatenate(predictions, axis=0)\n",
    "#     labels = np.concatenate(labels, axis=0)\n",
    "#     predictions = (predictions > 0.5).astype(int)\n",
    "#     acc = accuracy_score(labels, predictions)\n",
    "#     f1 = f1_score(labels, predictions, average='macro')\n",
    "#     return predictions, labels, acc, f1\n",
    "\n",
    "# # Evaluate the model\n",
    "# predictions, labels, accuracy, f1 = evaluate_model(trainer, test_dataset)\n",
    "# print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "# # Classification report\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(labels, predictions, target_names=frequent_diseases))\n",
    "\n",
    "# # Plot all ROC curves in one graph\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# for i, label in enumerate(frequent_diseases):\n",
    "#     fpr, tpr, _ = roc_curve(labels[:, i], predictions[:, i])\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "#     plt.plot(fpr, tpr, lw=2, label=f'ROC curve of {label} (area = {roc_auc:.2f})')\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic for all labels')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
